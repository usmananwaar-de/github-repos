{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7429e3-1ddc-4f49-b92f-cfc37f413002",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Objective 1\n",
    "\n",
    "Get a list of the last 1M repos from Github, load data into a warehouse, and run an exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350ac516-56a0-476c-a953-f14ca6dd634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, logging, json, os\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# CONFIGURING LOGGING\n",
    "logging.basicConfig(filename= 'github_api_logs.log', level= logging.DEBUG)\n",
    "\n",
    "def get_repo_data(url, headers):\n",
    "    try:\n",
    "        # MAKING A GET REQUEST TO THE API\n",
    "        response= requests.get(url, headers= headers)\n",
    "        logging.info(\"Making an API call\")\n",
    "        \n",
    "        # WE CAN USE PAGINATION TO GET MORE REPO DATA. I DIDN't USE PAGINATION HERE SINCE GITHUB PLACES A LIMIT ON THE NUMBER OF API CALLS YOU CAN MAKE, \n",
    "        # AND I DIDN'T KNOW HOW MUCH DATA THERE WOULD BE, SO I RESTRICTED THE RESULTS TO ONLY 100 REPOSITORIES.\n",
    "\n",
    "        # CONVERT JSON RESPONSE INTO PYTHON DICTIONARY\n",
    "        data= response.json()\n",
    "        logging.info(f\"API call successful\")\n",
    "        return data\n",
    "    \n",
    "    # IF ERROR\n",
    "    except requests.exceptions.HTTPError as err_http:\n",
    "        logging.error(f\"HTTP Error: {err_http}\")\n",
    "        return None\n",
    "    except requests.exceptions.ConnectionError as err_conn:\n",
    "        logging.error(f\"Error Connecting: {err_conn}\")\n",
    "        return None\n",
    "    except requests.exceptions.Timeout as err_timeout:\n",
    "        logging.error(f\"Timeout Error: {err_timeout}\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        logging.error(f\"Something went wrong: {err}\")\n",
    "        return None\n",
    "\n",
    "# EXTRACTING REPOSITORIES URL FROM RESPONSE AND APPENDING THEM TO THE LIST\n",
    "def get_repo_url(data):\n",
    "    url_lst= []\n",
    "    logging.info(\"Empty List Created for URL\")\n",
    "    try:\n",
    "        for num in range(len(data)):\n",
    "            url_lst.append(data[num]['url'])\n",
    "        logging.info(\"URLs appended\")\n",
    "        return url_lst\n",
    "    except Exception as err:\n",
    "        logging.error(f\"Can't get data. Error: {err}\")\n",
    "\n",
    "# REQUESTING DATA FROM REPO URLs        \n",
    "def get_data(urls):\n",
    "    repo_data= []\n",
    "    logging.info(\"Empty List Created for REPO Data\")\n",
    "    \n",
    "    for url in urls:\n",
    "        response= requests.get(url)\n",
    "        data = response.json()\n",
    "        repo_data.append(data)\n",
    "    logging.info(\"Repo Data Appended\")\n",
    "    # CONVERTING PYTHON LIST TO JSON DOCUMENT\n",
    "    repo_data= json.dumps(repo_data)\n",
    "    return repo_data\n",
    "\n",
    "def json_to_df(repo_data):\n",
    "    \n",
    "    # KEYS LIST TO EXTRACT THEIR DATA FROM JSON WHILE CONVERTING INTO DATAFRAME\n",
    "    keys = ['name', 'html_url', 'created_at', 'updated_at', 'language', 'watchers', 'forks', 'open_issues', 'subscribers_count']\n",
    "    df = pd.DataFrame.from_dict(repo_data)\n",
    "    df = df[keys]\n",
    "    logging.info(\"Converted into Pandas Dataframe\")\n",
    "    return df\n",
    "\n",
    "def remove_null(df):\n",
    "    # REMOVING ALL RECORDS WITH NULL VALUES. HALF OF THE COLUMNS CONTAIN NULL VALUES AND IT IS BECAUSE GIT SERVER FORBIDDEN CALLS FROM MY API: \n",
    "    # DEBUG:urllib3.connectionpool:https://api.github.com:443 \"GET /repos/sr/tasks HTTP/1.1\" 403 279 DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.github.com:443\n",
    "    df= df.dropna(how= 'all')\n",
    "    logging.info('Dropping all columns')\n",
    "    return df\n",
    "\n",
    "# SENDING JSON DATA TO GOOGLE BIGQUERY\n",
    "def df_to_gbq(df):\n",
    "    try:\n",
    "        # SERVICE ACCOUNT KEY \n",
    "        credentials = service_account.Credentials.from_service_account_file(\n",
    "            'spiritual-slate-374706-a81759778bb5.json',\n",
    "            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    "        )\n",
    "        logging.info('Connection Successful')\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Couldn't establish connection with BigQuery. Error: {e}\")\n",
    "    # UPLOADING DATAFRAME TO BIGQUERY\n",
    "    df.to_gbq('github.repos', project_id='spiritual-slate-374706', if_exists='append', credentials=credentials)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    headers = {\n",
    "    \"Accept\" : \"application/vnd.github+json\",\n",
    "    \"Authorization\" : \"Bearer <Token>\", # Write your github api\n",
    "    \"X-GitHub-Api-Version\" : \"2022-11-28\"\n",
    "    }\n",
    "    url= 'https://api.github.com/repositories'\n",
    "    data= get_repo_data(url, headers)\n",
    "    url_list= get_repo_url(data)\n",
    "    repo_data= get_data(url_list)\n",
    "    df= json_to_df(json.loads(repo_data))\n",
    "    df= remove_null(df)\n",
    "    df_to_gbq(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
